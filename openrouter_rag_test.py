import os
import csv
import json
import pickle
from langchain.schema import Document
from langchain.vectorstores import FAISS
from langchain.embeddings.base import Embeddings
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun
from openai import OpenAI

# =============== ØªÙ†Ø¸ÛŒÙ…Ø§Øª OpenRouter Ø¨Ø§ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ openai ===============
os.environ["OPENROUTER_API_KEY"] = "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"

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=os.environ["OPENROUTER_API_KEY"]
)

MODEL_NAME = "deepseek/deepseek-r1:free"  # ÛŒØ§ Ù…Ø¯Ù„ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø¯ÛŒÚ¯Ø±

# =============== Ú©Ù„Ø§Ø³ Embedding Ø¨Ø§ LLM ===============
class OpenRouterPromptEmbedding(Embeddings):
    def embed_documents(self, texts):
        return [self._embed_text(t) for t in texts]
    
    def embed_query(self, text):
        return self._embed_text(text)
    
    def _embed_text(self, text):
        prompt = f"Ù…ØªÙ† Ø²ÛŒØ± Ø±Ø§ Ø¨Ù‡ ÛŒÚ© Ø¨Ø±Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ 512 Ø¨ÙØ¹Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù† Ùˆ ÙÙ‚Ø· JSON Ù„ÛŒØ³Øª Ø§Ø¹Ø¯Ø§Ø¯ Ø±Ø§ Ø¨Ø¯Ù‡:\n{text}"
        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}]
        )
        try:
            vector = json.loads(resp.choices[0].message.content)
        except:
            vector = [0.0] * 512
        return vector

# =============== Ú©Ù„Ø§Ø³ LLM Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® ===============
class OpenRouterLLM(LLM):
    def _call(self, prompt: str, stop=None, run_manager: CallbackManagerForLLMRun = None):
        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "ØªÙˆ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ú©Ø§ÙÙ‡ Ù‡Ø³ØªÛŒ Ú©Ù‡ Ù…Ø´ØªØ±ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù†ÙˆÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ù…Ø­ØªØ±Ù…Ø§Ù†Ù‡ Ùˆ Ú©ÙˆØªØ§Ù‡ Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡."},
                {"role": "user", "content": prompt}
            ]
        )
        return resp.choices[0].message.content
    
    @property
    def _identifying_params(self):
        return {"model": MODEL_NAME}
    
    @property
    def _llm_type(self):
        return "openrouter_llm"

# =============== Ø³Ø§Ø®Øª ÛŒØ§ Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ ===============
INDEX_FILE = "cafe_faiss.pkl"
embedding_fn = OpenRouterPromptEmbedding()

if os.path.exists(INDEX_FILE):
    print("ğŸ“‚ Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¢Ù…Ø§Ø¯Ù‡...")
    with open(INDEX_FILE, "rb") as f:
        vectorstore = pickle.load(f)
else:
    print("âš™ï¸ Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø±Ø¯Ø§Ø± Ø§Ø² CSV...")
    docs = []
    with open("cafe_menu.csv", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            content = " - ".join([f"{k}: {v}" for k, v in row.items()])
            docs.append(Document(page_content=content))
    
    vectorstore = FAISS.from_documents(docs, embedding_fn)
    
    with open(INDEX_FILE, "wb") as f:
        pickle.dump(vectorstore, f)

# =============== Ø¬Ø³ØªØ¬Ùˆ Ùˆ Ù¾Ø§Ø³Ø® ===============
llm = OpenRouterLLM()

while True:
    query = input("\nğŸµ Ø³ÙˆØ§Ù„ Ø´Ù…Ø§: ")
    if query.lower() in ["exit", "quit", "Ø®Ø±ÙˆØ¬"]:
        break
    
    results = vectorstore.similarity_search(query, k=3)
    context = "\n".join([doc.page_content for doc in results])
    
    final_prompt = f"""
Ù…Ù†ÙˆÛŒ Ú©Ø§ÙÙ‡:
{context}

Ø³ÙˆØ§Ù„ Ù…Ø´ØªØ±ÛŒ: {query}
"""
    answer = llm(final_prompt)
    print("ğŸ¤– Ù¾Ø§Ø³Ø®:", answer)
