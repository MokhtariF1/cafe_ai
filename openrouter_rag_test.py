import os
import csv
import pickle
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun
from openai import OpenAI

# ================= ØªÙ†Ø¸ÛŒÙ…Ø§Øª =================
os.environ["OPENROUTER_API_KEY"] = "Ú©Ù„ÛŒØ¯_API_Ø®ÙˆØ¯Øª"

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=os.environ["OPENROUTER_API_KEY"]
)

MODEL_NAME = "openai/gpt-3.5-turbo"  # Ù…Ø¯Ù„ LLM Ø§Ø² OpenRouter
INDEX_FILE = "cafe_faiss.pkl"
CSV_FILE = "cafe_menu.csv"

# ================= Ú©Ù„Ø§Ø³ LLM Ø§Ø² OpenRouter =================
class OpenRouterLLM(LLM):
    def _call(self, prompt: str, stop=None, run_manager: CallbackManagerForLLMRun = None):
        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "ØªÙˆ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ú©Ø§ÙÙ‡ Ù‡Ø³ØªÛŒ Ú©Ù‡ Ù…Ø´ØªØ±ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù†ÙˆÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ù…Ø­ØªØ±Ù…Ø§Ù†Ù‡ Ùˆ Ú©ÙˆØªØ§Ù‡ Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡."},
                {"role": "user", "content": prompt}
            ]
        )
        return resp.choices[0].message.content
    
    @property
    def _identifying_params(self):
        return {"model": MODEL_NAME}
    
    @property
    def _llm_type(self):
        return "openrouter_llm"

# ================= Ø³Ø§Ø®Øª ÛŒØ§ Ù„ÙˆØ¯ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ =================
embedding_fn = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

if os.path.exists(INDEX_FILE):
    print("ğŸ“‚ Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¢Ù…Ø§Ø¯Ù‡...")
    with open(INDEX_FILE, "rb") as f:
        vectorstore = pickle.load(f)
else:
    print("âš™ï¸ Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø±Ø¯Ø§Ø± Ø§Ø² CSV...")
    docs = []
    with open(CSV_FILE, encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            content = " - ".join([f"{k}: {v}" for k, v in row.items()])
            docs.append(Document(page_content=content))
    
    vectorstore = FAISS.from_documents(docs, embedding_fn)
    
    with open(INDEX_FILE, "wb") as f:
        pickle.dump(vectorstore, f)

# ================= Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø® =================
llm = OpenRouterLLM()

while True:
    query = input("\nğŸµ Ø³ÙˆØ§Ù„ Ø´Ù…Ø§: ")
    if query.lower() in ["exit", "quit", "Ø®Ø±ÙˆØ¬"]:
        break
    
    results = vectorstore.similarity_search(query, k=3)
    context = "\n".join([doc.page_content for doc in results])
    
    final_prompt = f"""
Ù…Ù†ÙˆÛŒ Ú©Ø§ÙÙ‡:
{context}

Ø³ÙˆØ§Ù„ Ù…Ø´ØªØ±ÛŒ: {query}
"""
    answer = llm(final_prompt)
    print("ğŸ¤– Ù¾Ø§Ø³Ø®:", answer)
