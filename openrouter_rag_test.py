import os
import csv
from typing import Optional, List, Any, Mapping
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.memory import ConversationBufferMemory
from pydantic import Field
from openai import OpenAI

# ===== ØªÙ†Ø¸ÛŒÙ…Ø§Øª =====
os.environ["OPENROUTER_API_KEY"] = "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"

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=os.environ["OPENROUTER_API_KEY"]
)

MODEL_NAME = "openrouter/horizon-beta"
CSV_FILE = "cafe_menu.csv"
FAISS_DIR = "cafe_faiss"

# ===== LLM Ø³ÙØ§Ø±Ø´ÛŒ Ø¨Ø§ Ø­Ø§ÙØ¸Ù‡ =====
class OpenRouterLLM(LLM):
    memory: ConversationBufferMemory = Field(default_factory=ConversationBufferMemory)

    def _call(self, prompt: str, stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None) -> str:
        # ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù‚Ø¨Ù„ÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡
        history = self.memory.load_memory_variables({})["history"]

        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "ØªÙˆ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ú©Ø§ÙÙ‡ Ù‡Ø³ØªÛŒ Ú©Ù‡ Ù…Ø´ØªØ±ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù†ÙˆÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ù…Ø­ØªØ±Ù…Ø§Ù†Ù‡ Ùˆ Ú©ÙˆØªØ§Ù‡ Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡."},
                {"role": "user", "content": history + "\n" + prompt}
            ]
        )

        answer = resp.choices[0].message.content
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡
        self.memory.chat_memory.add_user_message(prompt)
        self.memory.chat_memory.add_ai_message(answer)
        return answer

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        return {"model": MODEL_NAME}

    @property
    def _llm_type(self) -> str:
        return "openrouter_llm"

# ===== Embedding Ø¨Ø§ Ollama =====
embedding_fn = OllamaEmbeddings(model="nomic-embed-text")

# ===== Ø³Ø§Ø®Øª ÛŒØ§ Ù„ÙˆØ¯ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ =====
if os.path.exists(FAISS_DIR):
    print("ğŸ“‚ Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¢Ù…Ø§Ø¯Ù‡...")
    vectorstore = FAISS.load_local(FAISS_DIR, embedding_fn, allow_dangerous_deserialization=True)
else:
    print("âš™ï¸ Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø±Ø¯Ø§Ø± Ø§Ø² CSV...")
    docs = []
    with open(CSV_FILE, encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            content = " - ".join([f"{k}: {v}" for k, v in row.items()])
            docs.append(Document(page_content=content))

    vectorstore = FAISS.from_documents(docs, embedding_fn)
    vectorstore.save_local(FAISS_DIR)

# ===== Ø§Ø¬Ø±Ø§ÛŒ Ú†Øª =====
llm = OpenRouterLLM()

while True:
    query = input("\nğŸµ Ø³ÙˆØ§Ù„ Ø´Ù…Ø§: ")
    if query.lower() in ["exit", "quit", "Ø®Ø±ÙˆØ¬"]:
        break

    results = vectorstore.similarity_search(query, k=3)
    context = "\n".join([doc.page_content for doc in results])

    final_prompt = f"""
Ù…Ù†ÙˆÛŒ Ú©Ø§ÙÙ‡:
{context}

Ø³ÙˆØ§Ù„ Ù…Ø´ØªØ±ÛŒ: {query}
"""
    answer = llm(final_prompt)
    print("ğŸ¤– Ù¾Ø§Ø³Ø®:", answer)
